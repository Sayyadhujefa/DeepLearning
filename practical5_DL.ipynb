{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMo33RlDFc5i5yY3nm0BEi9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"R5C-ZyJWCLE-","executionInfo":{"status":"ok","timestamp":1761828106409,"user_tz":-330,"elapsed":5301,"user":{"displayName":"Wagh Aryan","userId":"13955394281687036499"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import skipgrams\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Embedding, Flatten\n","import numpy as np\n","import collections"]},{"cell_type":"code","source":["# --- a. Data Preparation ---\n","# Sample corpus\n","corpus = \"\"\"\n","The quick brown fox jumps over the lazy dog.\n","The dog barks, and the fox runs away.\n","A quick brown rabbit also jumps.\n","\"\"\""],"metadata":{"id":"uxcgfl_0CXQp","executionInfo":{"status":"ok","timestamp":1761828225539,"user_tz":-330,"elapsed":3,"user":{"displayName":"Wagh Aryan","userId":"13955394281687036499"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Tokenize the corpus\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([corpus])\n","word_index = tokenizer.word_index\n","vocab_size = len(word_index) + 1 # +1 for padding/unknown words\n","print(f\"Vocabulary: {word_index}\")\n","print(f\"Vocabulary size: {vocab_size}\")\n","# Convert corpus to sequences of integers\n","sequences = tokenizer.texts_to_sequences([corpus])[0]\n","print(f\"Sequences: {sequences}\")\n","# --- b. Generate Training Data (Skip-gram is used here for demonstration, CBOW concept is similar) ---\n","# For CBOW, we'd typically create pairs of (context_words, target_word)\n","# Here, we use skipgrams for simplicity as it generates pairs from sequences.\n","# skipgrams returns pairs of (target_word, context_word)\n","# Parameters for skipgrams\n","window_size = 2 # Context window size\n","# Note: For true CBOW, you'd structure this differently, but skipgrams is often used to generate pairs for word2vec-like models.\n","# We will generate pairs of (target_word, context_word) using skipgrams for now,\n","# and then adapt it to mimic CBOW's prediction goal.\n","# A typical CBOW implementation involves averaging context embeddings.\n","# A simplified approach to get CBOW data structure:"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K3yNvJftCZ3j","executionInfo":{"status":"ok","timestamp":1761828242407,"user_tz":-330,"elapsed":18,"user":{"displayName":"Wagh Aryan","userId":"13955394281687036499"}},"outputId":"7807e138-5c99-4bbb-8704-78cb5ebd6cc5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary: {'the': 1, 'quick': 2, 'brown': 3, 'fox': 4, 'jumps': 5, 'dog': 6, 'over': 7, 'lazy': 8, 'barks': 9, 'and': 10, 'runs': 11, 'away': 12, 'a': 13, 'rabbit': 14, 'also': 15}\n","Vocabulary size: 16\n","Sequences: [1, 2, 3, 4, 5, 7, 1, 8, 6, 1, 6, 9, 10, 1, 4, 11, 12, 13, 2, 3, 14, 15, 5]\n"]}]},{"cell_type":"code","source":["# For each word, gather its context.\n","# Example: \"The quick brown fox\" -> target: \"brown\", context: [\"The\", \"quick\", \"fox\"]\n","data = []\n","target = []\n","context_window = 2\n","# Iterate through the sequences\n","for i, word in enumerate(sequences):\n","      context_start = max(0, i - context_window)\n","      context_end = min(len(sequences), i + context_window + 1)\n","      context = sequences[context_start:i] + sequences[i+1:context_end]\n","# Ensure context is not empty\n","if context:\n","# For CBOW, we want to predict the word 'word' from 'context'\n","# We can represent context by averaging its embeddings later, or use its indices\n","# Let's prepare data where input is context indices, and target is the word index\n","# For simplicity in Keras, we might one-hot encode contexts or average embeddings\n","# A common simplification for Keras is to use a multi-hot encoding of context\n","# or directly use skipgrams pairs and adapt the model.\n","# Let's stick to generating data in a way that the model can learn the prediction.\n","# A typical CBOW structure in Keras: Input (context word indices) -> Embedding ->Average -> Dense -> Output\n","# For demonstration, let's create pairs where target is 'word' and input is a context word from 'context'\n","# This is closer to Skip-gram but can be adapted.\n","# To truly implement CBOW, one would average context embeddings.\n","# We'll use a simplified model that predicts target from individual context words, then can be modified.\n","# Let's generate pairs of (context_word_index, target_word_index)\n","    for context_word_index in context:\n","        data.append(context_word_index)\n","        target.append(word)"],"metadata":{"id":"tDwJ901JCdRv","executionInfo":{"status":"ok","timestamp":1761828306162,"user_tz":-330,"elapsed":12,"user":{"displayName":"Wagh Aryan","userId":"13955394281687036499"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"o4UC_hGOC247"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert lists to numpy arrays\n","data = np.array(data)\n","target = np.array(target)\n","print(f\"\\nGenerated context-target pairs (simplified): {len(data)} pairs\")\n","# print(f\"Example pair (context_word_index, target_word_index): ({data[0]}, {target[0]})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNLaVIjQC-V1","executionInfo":{"status":"ok","timestamp":1761828310817,"user_tz":-330,"elapsed":44,"user":{"displayName":"Wagh Aryan","userId":"13955394281687036499"}},"outputId":"7e8964d5-4028-404e-e683-ee33b7057b47"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Generated context-target pairs (simplified): 2 pairs\n"]}]},{"cell_type":"code","source":["# --- c. Train Model ---\n","embedding_dim = 10 # Dimension of the word embeddings\n","# CBOW Model: Predict target word from context words\n","# Input layer will take context word index\n","context_input = Input(shape=(1,), name='context_input')\n","# Embedding layer: Maps word indices to dense vectors\n","embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n","name='word_embedding')\n","context_embedding = embedding_layer(context_input) # Embeddings for context words\n","# We need to aggregate context embeddings. In true CBOW, we average them.\n","# For Keras, this is often done implicitly or by custom layers."],"metadata":{"id":"DL5kTBiODBSy","executionInfo":{"status":"ok","timestamp":1761828312847,"user_tz":-330,"elapsed":32,"user":{"displayName":"Wagh Aryan","userId":"13955394281687036499"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# A simpler model might predict target from each context word individually, then average results,\n","# or use a multi-hot encoding for context and average after embedding.\n","# Let's use a simplified model: input is ONE context word, predict the target.\n","# This is more akin to skip-gram, but can illustrate the embedding learning.\n","# For a true CBOW, you'd process all context words together.\n","# Output layer: Predicts the target word (softmax over vocabulary)\n","output_layer = Dense(vocab_size, activation='softmax', name='output_layer')\n","output_probs = output_layer(context_embedding)\n","# Create the model\n","# This simplified model predicts target from a SINGLE context word.\n","# To make it CBOW, you'd need to:\n","# 1. input multiple context words (e.g., a sequence of indices)\n","# 2. pass them through the embedding layer\n","# 3. average the resulting embeddings\n","# 4. pass the averaged embedding through a Dense layer\n","# Let's adapt to predict target from averaging context embeddings.\n","# This requires a slightly more complex setup or a pre-processing step.\n","# For a practical Keras CBOW:\n","# 1. Prepare data where each training instance has multiple context word indices.\n","# 2. Input shape would be (num_context_words,)\n","# 3. Use an Embedding layer.\n","# 4. Use a Lambda layer to average the embeddings of context words.\n","# 5. Then feed into Dense output."],"metadata":{"id":"FsIUFm9RDWIY","executionInfo":{"status":"ok","timestamp":1761828319880,"user_tz":-330,"elapsed":13,"user":{"displayName":"Wagh Aryan","userId":"13955394281687036499"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Re-preparing data for actual CBOW structure:\n","# For each word, gather its context indices.\n","# Input: [context_word1_idx, context_word2_idx, ...]\n","# Target: target_word_idx\n","data_cbow = []\n","target_cbow = []\n","for i, word in enumerate(sequences):\n","      context_start = max(0, i - context_window)\n","      context_end = min(len(sequences), i + context_window + 1)\n","      context_indices = sequences[context_start:i] + sequences[i+1:context_end]\n","if context_indices:\n","# Pad context if needed to have a fixed length, or handle variable length\n","# For simplicity, let's assume a fixed context window size that we can pad\n","# Here, we'll use the actual context and average.\n","      data_cbow.append(context_indices)\n","      target_cbow.append(word)\n","# Convert to numpy arrays\n","data_cbow = np.array(data_cbow)\n","target_cbow = np.array(target_cbow)\n","print(f\"\\nCBOW data shape: {data_cbow.shape}\") # (num_samples, avg_num_context_words)\n","print(f\"CBOW target shape: {target_cbow.shape}\") # (num_samples,)\n","# Building the CBOW model architecture"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WAk-H37YDcAe","executionInfo":{"status":"ok","timestamp":1761828358305,"user_tz":-330,"elapsed":25,"user":{"displayName":"Wagh Aryan","userId":"13955394281687036499"}},"outputId":"bba57de8-73ee-429b-dec6-5cb970d8dc06"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","CBOW data shape: (1, 2)\n","CBOW target shape: (1,)\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.layers import Lambda\n","import tensorflow.keras.backend as K\n","\n","# --- CBOW model architecture ---\n","cbow_input = Input(shape=(None,), name='cbow_input')  # variable number of context words\n","\n","# Embedding layer\n","embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name='word_embedding')\n","embedded_contexts = embedding(cbow_input)  # (batch_size, num_context_words, embedding_dim)\n","\n","# Average the embeddings\n","def average_embeddings(x):\n","    return K.mean(x, axis=1)\n","\n","averaged_context = Lambda(average_embeddings, output_shape=(embedding_dim,), name='average_context')(embedded_contexts)\n","\n","# Output layer — THIS is where you made the mistake before\n","cbow_output = Dense(vocab_size, activation='softmax', name='output_layer')(averaged_context)\n","\n","# Build and compile the model\n","cbow_model = Model(inputs=cbow_input, outputs=cbow_output)\n","cbow_model.compile(optimizer='adam',\n","                   loss='sparse_categorical_crossentropy',\n","                   metrics=['accuracy'])\n","\n","cbow_model.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":257},"id":"FYYKPjouDenw","executionInfo":{"status":"ok","timestamp":1761828740033,"user_tz":-330,"elapsed":74,"user":{"displayName":"Wagh Aryan","userId":"13955394281687036499"}},"outputId":"cd7af3a5-afcd-425a-f020-de21850d77cc"},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ cbow_input (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ word_embedding (\u001b[38;5;33mEmbedding\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)       │           \u001b[38;5;34m160\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ average_context (\u001b[38;5;33mLambda\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m176\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ cbow_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ word_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ average_context (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">176</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m336\u001b[0m (1.31 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">336</span> (1.31 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m336\u001b[0m (1.31 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">336</span> (1.31 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"code","source":["# Create the CBOW model\n","cbow_model = Model(inputs=cbow_input, outputs=cbow_output)\n","cbow_model.compile(optimizer='adam',\n","loss='sparse_categorical_crossentropy', # Use sparse because target is integer index\n","metrics=['accuracy'])"],"metadata":{"id":"tZ8LF3mDDh64","executionInfo":{"status":"ok","timestamp":1761828743707,"user_tz":-330,"elapsed":8,"user":{"displayName":"Wagh Aryan","userId":"13955394281687036499"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["cbow_model.summary()\n","# Train the CBOW model\n","print(\"\\nTraining the CBOW model...\")\n","# You might need to pad sequences if using fixed input shapes or handle variable lengths.\n","# For 'None' input shape, Keras handles variable lengths automatically for this setup.\n","history_cbow = cbow_model.fit(data_cbow, target_cbow, epochs=100, batch_size=16,\n","verbose=0) # Increased epochs for better learning\n","# --- d. Output ---\n","print(\"\\nTraining finished. Extracting word embeddings.\")\n","# Extract the word embeddings from the embedding layer\n","word_embeddings = cbow_model.get_layer('word_embedding').get_weights()[0]\n","print(f\"Shape of word embeddings: {word_embeddings.shape}\") # (vocab_size, embedding_dim)\n","# Display embeddings for a few words\n","print(\"\\nWord Embeddings:\")\n","for word, i in word_index.items():\n","    print(f\"'{word}': {word_embeddings[i][:5]}...\") # Print first 5 dimensions\n","# You can now use these embeddings for similarity calculation, etc.\n","# For example, finding words similar to 'fox':\n","# Calculate cosine similarity between 'fox' vector and all other vectors."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":639},"id":"xmgY5qplDmeD","executionInfo":{"status":"ok","timestamp":1761828778149,"user_tz":-330,"elapsed":3801,"user":{"displayName":"Wagh Aryan","userId":"13955394281687036499"}},"outputId":"225486b3-accd-4a49-b384-388bb8c66333"},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"functional_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ cbow_input (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ word_embedding (\u001b[38;5;33mEmbedding\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)       │           \u001b[38;5;34m160\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ average_context (\u001b[38;5;33mLambda\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m176\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ cbow_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ word_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ average_context (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">176</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m336\u001b[0m (1.31 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">336</span> (1.31 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m336\u001b[0m (1.31 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">336</span> (1.31 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Training the CBOW model...\n","\n","Training finished. Extracting word embeddings.\n","Shape of word embeddings: (16, 10)\n","\n","Word Embeddings:\n","'the': [ 0.02066291  0.00528953  0.01402289 -0.00101294 -0.04050182]...\n","'quick': [ 0.01566211  0.03506346  0.03470664  0.03439732 -0.03385506]...\n","'brown': [ 0.01559262 -0.02316748 -0.03885341 -0.01975107 -0.03973562]...\n","'fox': [-0.00189878  0.02852105  0.03664095  0.01970431 -0.01887131]...\n","'jumps': [-0.01504601 -0.02598949  0.01867386  0.00542425  0.03774353]...\n","'dog': [-0.03331993 -0.04652214 -0.02331786  0.01824592  0.03961295]...\n","'over': [-0.03141618 -0.02075325 -0.04179676 -0.02061243  0.00206099]...\n","'lazy': [-0.03564785 -0.03857515  0.03767255  0.01030811  0.01026169]...\n","'barks': [-0.01016742 -0.01193502  0.02322466 -0.00797284 -0.02806881]...\n","'and': [-0.04502343  0.04186846  0.04536377  0.01505884  0.02160091]...\n","'runs': [ 0.04338505 -0.01640583  0.01897473  0.00752043  0.00414664]...\n","'away': [ 0.0410695   0.04887905 -0.04533111 -0.01337685 -0.01348662]...\n","'a': [ 0.00622248 -0.02738328  0.04084292  0.04930563 -0.04555526]...\n","'rabbit': [-0.07831626 -0.14404954  0.11438333 -0.10290396  0.04868127]...\n","'also': [-0.11068997 -0.10183968  0.14109285 -0.07370432  0.05529835]...\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"q777Hfz7LaBq"},"execution_count":null,"outputs":[]}]}